= 4. Deploying a model - 15 minutes
:imagesdir: ../assets/images

After this overview of how a model can be trained in RHODS, we're going to see how you can serve it, which will make it easier to use later on in our intelligent application.

RHODS integrates the https://docs.openvino.ai/latest/ovms_what_is_openvino_model_server.html[Intel's OpenVino Model Server^] runtime, a high-performance system for serving models, optimized for deployment on Intel architectures.

As we said before, the model has been saved in the https://onnx.ai/[ONNX^] format, an open standard for machine learning interoperability, which is one we can use with OpenVino and RHODS model serving. The model has been stored and is available for download in an S3-compatible object store bucket.

Let's serve this model!

== 4.1. Check the model location

There are many ways and different tools available to interact with Object Storage. Here we are going to use the Python library named `boto3`.

* In your project in JupyterLab, open the notebook `02_check_model.ipynb` and follow the instructions to check the availability and the exact location of the model. Don't forget to run the cells!

image::check_model.png[]

* Once you are finished, you can come back to these instructions.

image::check_model_complete.png[]

== 4.2. Deploy the model in RHODS

With your model available on S3 storage, it is pretty easy to serve it.

* Switch back to the OpenShift Data Science dashboard. If you have closed it, you can access it https://rhods-dashboard-redhat-ods-applications.%SUBDOMAIN%[here^] or you can reopen it from Jupyter by clicking on **File->Hub Control Panel**:

image::hub_panel.png[]

* In the RHODS dashboard, still in your Data Science Project, create a Data connection. It will serve as a reference and configuration for access to the object bucket. In the **Data connections** section, select **Add data connection**:

image::add_data_connection.png[]

* Fill in the information, the select **Add data connection**:
    ** Name it `Crops`
    ** `AWS_ACCESS_KEY_ID` is `odsceast`.
    ** `AWS_SECRET_ACCESS_KEY` is `odsceast`.
    ** `AWS_S3_ENDPOINT` is `http://s3.redhat-ods-applications.svc.cluster.local:9000`.
    ** `AWS_DEFAULT_REGION` is `us-east-1` (default).
    ** `AWS_S3_BUCKET` is `models`.

No need to specify any connected workbench, this data connection will only be used by the model server.

image::data_connection_configuration.png[]

* You now have a data connection ready to use:

image::data_connection_ok.png[]

* Now, on the **Models and model servers** section select **Configure server**:

image::configure_model_server.png[]

* Leave replicas to 1, size to Small. At this point, `don't` check **Make model available via an external route**. Then select **Configure**:

image::model_server_configure.png[]

* We now have a model server available (but no model deployed yet). Select **Deploy model**:

image::model_server_available.png[]

* Give a name to your model, `crops`, select **onnx - 1** for the framework, select the Data location you created before for the Model location, and enter **crops** as the folder path for the model as this is the location that we have seen before, then select **Deploy**:

image::deploy_model_configuration.png[]

NOTE: It's very important to respect the naming of the model here, `crops`. Otherwise some files won't work later on.

* The model is now deploying. You may have click on the `1` on the `Deployed models` tab to see details:

image::deployed_models.png[]

* After about 30 seconds to 1 minute, under the **Status** column, you should see a green tick to the right of you model, indicating it's ready for use!

image::model_ready.png[]

* Once it's ready, select the **Internal Service** link under the Inference endpoint column, and make note of the restUrl value, we will need it later. It should be `http://modelmesh-serving.%USERID%:8033`

image::resturl.png[]

== 4.3. Test the model

Now that the model server is ready to receive requests, we can test it.

* In your project in JupyterLab, open the notebook `03_test_model_serving.ipynb`, follow the instructions and run all the cells to see how the model can be queried.

image::remote_inference.png[]

* Once you complete the notebook's instruction, you will end up with this result.

image::remote_inference_complete.png[]

Perfect! We now have a working model server. But we need a some more tools for our application. Let's head to the next section!